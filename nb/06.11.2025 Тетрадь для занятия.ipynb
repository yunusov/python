{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f168d4",
   "metadata": {},
   "source": [
    "## Тема 1.6. Сбор данных. Лекция: Сбор данных в Internet\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Введение\n",
    "\n",
    "Сбор данных в интернете (web scraping, веб-скрапинг) — это процесс автоматизированного получения информации с веб-сайтов. Эта технология широко используется для создания агрегаторов, аналитики рынка, мониторинга цен, создания информационных баз и во многих других задачах.\n",
    "\n",
    "### 2. Основные методы сбора данных в интернете\n",
    "\n",
    "1. **Загрузка файлов напрямую по ссылке** (например, CSV, XLS, TXT, HTML).\n",
    "2. **Работа с API** — если сайт предоставляет программный интерфейс (API), лучше использовать его.\n",
    "3. **Веб-скрапинг** (извлечение данных непосредственно с HTML-страниц).\n",
    "\n",
    "**ВНИМАНИЕ:** Всегда изучайте условия использования сайта! Некоторые сайты запрещают автоматизированный сбор данных.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Базовые инструменты на Python\n",
    "\n",
    "- **requests** — для скачивания веб-страниц.\n",
    "- **BeautifulSoup** — для парсинга HTML-кода.\n",
    "- **pandas** — для хранения и обработки собранных данных.\n",
    "- **re (регулярные выражения)** — полезны для извлечения структурированных данных из текста.\n",
    "\n",
    "#### Установка зависимостей\n",
    "\n",
    "```bash\n",
    "pip install requests beautifulsoup4 pandas\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Сбор HTML-страницы с помощью requests\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "url = 'https://example.com'\n",
    "response = requests.get(url)\n",
    "\n",
    "print(f'Код ответа: {response.status_code}')  # 200 - OK\n",
    "\n",
    "print(response.text[:500])  # первые 500 символов HTML\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Парсинг HTML с помощью BeautifulSoup\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Получение заголовка страницы\n",
    "title = soup.title.text\n",
    "print(f'Заголовок: {title}')\n",
    "\n",
    "# Получение всех ссылок с сайта\n",
    "links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "print(links)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Пример: Парсинг таблицы с сайта\n",
    "\n",
    "Допустим, на странице есть HTML-таблица с данными. Вот пример кода для её извлечения:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://translated.turbopages.org/proxy_u/en-ru.ru.7d534d03-68f763f4-f9cd49da-74722d776562/https/en.wikipedia.org/wiki/Table_(information)'\n",
    "resp = requests.get(url)\n",
    "soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "table = soup.find('table')\n",
    "\n",
    "# Считываем строки таблицы\n",
    "rows = []\n",
    "for tr in table.find_all('tr'):\n",
    "    cols = [td.get_text(strip=True) for td in tr.find_all(['td','th'])]\n",
    "    if cols:\n",
    "        rows.append(cols)\n",
    "\n",
    "# Создаём DataFrame из таблицы\n",
    "df = pd.DataFrame(rows[1:], columns=rows[0])\n",
    "print(df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Работа с API для сбора данных\n",
    "\n",
    "Когда сайт предоставляет API, сбор данных проще и юридически корректнее:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "CITY = \"Moscow\"\n",
    "API_KEY = \"c86250163953d321b9b61edeb46ad4c7\"\n",
    "url = f\"http://api.openweathermap.org/data/2.5/weather\"\n",
    "params = {\n",
    "    'q': CITY,\n",
    "    'appid': API_KEY,\n",
    "    'units': 'metric',  # для температуры в градусах Цельсия\n",
    "    'lang': 'ru'        # для русского языка\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "print(response.json())\n",
    "```\n",
    "\n",
    "Преобразование в табличный вид\n",
    "\n",
    "```python\n",
    "cities = ['Moscow', 'London', 'Paris', 'Vladivostok', 'Barselona']\n",
    "rows = []\n",
    "\n",
    "for city in cities:\n",
    "    params['q'] = city\n",
    "    r = requests.get(url, params=params)\n",
    "    d = r.json()\n",
    "    row = {\n",
    "        'city': d['name'],\n",
    "        'datetime': pd.to_datetime(d['dt'], unit='s'),\n",
    "        'temp': d['main']['temp'],\n",
    "        'humidity': d['main']['humidity'],\n",
    "        'pressure': d['main']['pressure'],\n",
    "        'wind_speed': d['wind']['speed'],\n",
    "        'weather': d['weather'][0]['description']\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Примеры ошибок и обработка\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "for page in range(1, 6):\n",
    "    try:\n",
    "        response = requests.get(f'https://example.com/page={page}', timeout=5)\n",
    "        print(response.status_code)\n",
    "        time.sleep(1)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Ошибка: {e}')\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
