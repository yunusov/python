{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f168d4",
   "metadata": {},
   "source": [
    "## Тема 1.6. Сбор данных. Лекция: Сбор данных в Internet\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Введение\n",
    "\n",
    "Сбор данных в интернете (web scraping, веб-скрапинг) — это процесс автоматизированного получения информации с веб-сайтов. Эта технология широко используется для создания агрегаторов, аналитики рынка, мониторинга цен, создания информационных баз и во многих других задачах.\n",
    "\n",
    "### 2. Основные методы сбора данных в интернете\n",
    "\n",
    "1. **Загрузка файлов напрямую по ссылке** (например, CSV, XLS, TXT, HTML).\n",
    "2. **Работа с API** — если сайт предоставляет программный интерфейс (API), лучше использовать его.\n",
    "3. **Веб-скрапинг** (извлечение данных непосредственно с HTML-страниц).\n",
    "\n",
    "**ВНИМАНИЕ:** Всегда изучайте условия использования сайта! Некоторые сайты запрещают автоматизированный сбор данных.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Базовые инструменты на Python\n",
    "\n",
    "- **requests** — для скачивания веб-страниц.\n",
    "- **BeautifulSoup** — для парсинга HTML-кода.\n",
    "- **pandas** — для хранения и обработки собранных данных.\n",
    "- **re (регулярные выражения)** — полезны для извлечения структурированных данных из текста.\n",
    "\n",
    "#### Установка зависимостей\n",
    "\n",
    "```bash\n",
    "pip install requests beautifulsoup4 pandas\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Сбор HTML-страницы с помощью requests\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "url = 'https://example.com'\n",
    "response = requests.get(url)\n",
    "\n",
    "print(f'Код ответа: {response.status_code}')  # 200 - OK\n",
    "\n",
    "print(response.text[:500])  # первые 500 символов HTML\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Парсинг HTML с помощью BeautifulSoup\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Получение заголовка страницы\n",
    "title = soup.title.text\n",
    "print(f'Заголовок: {title}')\n",
    "\n",
    "# Получение всех ссылок с сайта\n",
    "links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "print(links)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Пример: Парсинг таблицы с сайта\n",
    "\n",
    "Допустим, на странице есть HTML-таблица с данными. Вот пример кода для её извлечения:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://translated.turbopages.org/proxy_u/en-ru.ru.7d534d03-68f763f4-f9cd49da-74722d776562/https/en.wikipedia.org/wiki/Table_(information)'\n",
    "resp = requests.get(url)\n",
    "soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "table = soup.find('table')\n",
    "\n",
    "# Считываем строки таблицы\n",
    "rows = []\n",
    "for tr in table.find_all('tr'):\n",
    "    cols = [td.get_text(strip=True) for td in tr.find_all(['td','th'])]\n",
    "    if cols:\n",
    "        rows.append(cols)\n",
    "\n",
    "# Создаём DataFrame из таблицы\n",
    "df = pd.DataFrame(rows[1:], columns=rows[0])\n",
    "print(df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Работа с API для сбора данных\n",
    "\n",
    "Когда сайт предоставляет API, сбор данных проще и юридически корректнее:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "CITY = \"Moscow\"\n",
    "API_KEY = \"c86250163953d321b9b61edeb46ad4c7\"\n",
    "url = f\"http://api.openweathermap.org/data/2.5/weather\"\n",
    "params = {\n",
    "    'q': CITY,\n",
    "    'appid': API_KEY,\n",
    "    'units': 'metric',  # для температуры в градусах Цельсия\n",
    "    'lang': 'ru'        # для русского языка\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "print(response.json())\n",
    "```\n",
    "\n",
    "Преобразование в табличный вид\n",
    "\n",
    "```python\n",
    "cities = ['Moscow', 'London', 'Paris', 'Vladivostok', 'Barselona']\n",
    "rows = []\n",
    "\n",
    "for city in cities:\n",
    "    params['q'] = city\n",
    "    r = requests.get(url, params=params)\n",
    "    d = r.json()\n",
    "    row = {\n",
    "        'city': d['name'],\n",
    "        'datetime': pd.to_datetime(d['dt'], unit='s'),\n",
    "        'temp': d['main']['temp'],\n",
    "        'humidity': d['main']['humidity'],\n",
    "        'pressure': d['main']['pressure'],\n",
    "        'wind_speed': d['wind']['speed'],\n",
    "        'weather': d['weather'][0]['description']\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Примеры ошибок и обработка\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "for page in range(1, 6):\n",
    "    try:\n",
    "        response = requests.get(f'https://example.com/page={page}', timeout=5)\n",
    "        print(response.status_code)\n",
    "        time.sleep(1)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Ошибка: {e}')\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b046edc4",
   "metadata": {},
   "source": [
    "## Задания\n",
    "### 1. Получите HTML-код главной страницы Википедии (`https://ru.wikipedia.org/wiki/Главная_страница`) и выведите первые 300 символов.\n",
    "**Ответ:**\n",
    "```python\n",
    "import requests\n",
    "r = requests.get('https://ru.wikipedia.org/wiki/Главная_страница')\n",
    "print(r.text[:300])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Выведите статус ответа сайта https://www.python.org.\n",
    "**Ответ:**\n",
    "```python\n",
    "print(requests.get('https://www.python.org').status_code)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Найдите текст заголовка (title) главной страницы GitHub.\n",
    "**Ответ:**\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "r = requests.get('https://github.com')\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "print(soup.title.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Получите список всех ссылок (href) на странице https://www.bbc.com/news.\n",
    "**Ответ:**\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "r = requests.get('https://www.bbc.com/news')\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "print(links)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Определите, есть ли на https://stackoverflow.com тег h1 и если да — выведите его текст.\n",
    "**Ответ:**\n",
    "```python\n",
    "soup = BeautifulSoup(requests.get('https://stackoverflow.com').text, 'html.parser')\n",
    "h1 = soup.find('h1')\n",
    "if h1: print(h1.text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Найдите все изображения на https://www.imdb.com/chart/top и выведите 5 первых их абсолютных ссылок.\n",
    "**Ответ:**\n",
    "```python\n",
    "r = requests.get('https://www.imdb.com/chart/top')\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "imgs = [img['src'] for img in soup.find_all('img', src=True)]\n",
    "print(imgs[:5])  # Возможно, придется обработать относительные пути.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Извлеките все подзаголовки (h3) главной страницы https://www.bbc.com и выведите 10 первых.\n",
    "**Ответ:**\n",
    "```python\n",
    "r = requests.get('https://www.bbc.com')\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "print([h.text.strip() for h in soup.find_all('h3')][:10])\n",
    "```\n",
    "\n",
    "### 8. Подключитесь к API hh.ru https://api.hh.ru/vacancies\n",
    "**Ответ:**  \n",
    "```python\n",
    "import requests\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "base_url = 'https://api.hh.ru/vacancies'\n",
    "params = {\n",
    "    'per_page': 100,\n",
    "    'page': 0\n",
    "}\n",
    "\n",
    "vacancies = []\n",
    "pages_number = 15\n",
    "\n",
    "for page in range(pages_number):\n",
    "    params['page'] = page\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        page_data = response.json()\n",
    "        vacancies.extend(page_data['items'])\n",
    "    else:\n",
    "        print(f\"Не удалось получить данные с страницы {page}\")\n",
    "        break\n",
    "\n",
    "df = pd.DataFrame(vacancies)\n",
    "df['region'] = df['area'].apply(lambda x: x['name'])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
